{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade gym --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import Brax and some helper modules\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import collections\n",
    "from datetime import datetime\n",
    "import functools\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Callable, Dict, Optional, Sequence\n",
    "\n",
    "import brax\n",
    "\n",
    "from brax import envs\n",
    "from brax.envs.wrappers import gym as gym_wrapper\n",
    "from brax.envs.wrappers import torch as torch_wrapper\n",
    "from brax.io import metrics\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "  \"\"\"Standard PPO Agent with GAE and observation normalization.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               policy_layers: Sequence[int],\n",
    "               entropy_cost: float,\n",
    "               discounting: float,\n",
    "               reward_scaling: float,\n",
    "               device: str):\n",
    "    super(Agent, self).__init__()\n",
    "\n",
    "    policy = []\n",
    "    for w1, w2 in zip(policy_layers, policy_layers[1:]):\n",
    "      policy.append(nn.Linear(w1, w2))\n",
    "      policy.append(nn.SiLU())\n",
    "    policy.pop()  # drop the final activation\n",
    "    self.policy = nn.Sequential(*policy)\n",
    "    self.num_steps = torch.zeros((), device=device)\n",
    "    self.running_mean = torch.zeros(policy_layers[0], device=device)\n",
    "    self.running_variance = torch.zeros(policy_layers[0], device=device)\n",
    "\n",
    "    self.entropy_cost = entropy_cost\n",
    "    self.discounting = discounting\n",
    "    self.reward_scaling = reward_scaling\n",
    "    self.lambda_ = 0.95\n",
    "    self.epsilon = 0.3\n",
    "    self.device = device\n",
    "\n",
    "  @torch.jit.export\n",
    "  def dist_create(self, logits):\n",
    "    \"\"\"Normal followed by tanh.\n",
    "\n",
    "    torch.distribution doesn't work with torch.jit, so we roll our own.\"\"\"\n",
    "    loc, scale = torch.split(logits, logits.shape[-1] // 2, dim=-1)\n",
    "    scale = F.softplus(scale) + .001\n",
    "    return loc, scale\n",
    "\n",
    "  @torch.jit.export\n",
    "  def dist_sample_no_postprocess(self, loc, scale):\n",
    "    return torch.normal(loc, scale)\n",
    "\n",
    "  @classmethod\n",
    "  def dist_postprocess(cls, x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "  @torch.jit.export\n",
    "  def dist_entropy(self, loc, scale):\n",
    "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
    "    entropy = 0.5 + log_normalized\n",
    "    entropy = entropy * torch.ones_like(loc)\n",
    "    dist = torch.normal(loc, scale)\n",
    "    log_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
    "    entropy = entropy + log_det_jacobian\n",
    "    return entropy.sum(dim=-1)\n",
    "\n",
    "  @torch.jit.export\n",
    "  def dist_log_prob(self, loc, scale, dist):\n",
    "    log_unnormalized = -0.5 * ((dist - loc) / scale).square()\n",
    "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
    "    log_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
    "    log_prob = log_unnormalized - log_normalized - log_det_jacobian\n",
    "    return log_prob.sum(dim=-1)\n",
    "\n",
    "  @torch.jit.export\n",
    "  def update_normalization(self, observation):\n",
    "    self.num_steps += observation.shape[0] * observation.shape[1]\n",
    "    input_to_old_mean = observation - self.running_mean\n",
    "    mean_diff = torch.sum(input_to_old_mean / self.num_steps, dim=(0, 1))\n",
    "    self.running_mean = self.running_mean + mean_diff\n",
    "    input_to_new_mean = observation - self.running_mean\n",
    "    var_diff = torch.sum(input_to_new_mean * input_to_old_mean, dim=(0, 1))\n",
    "    self.running_variance = self.running_variance + var_diff\n",
    "\n",
    "  @torch.jit.export\n",
    "  def normalize(self, observation):\n",
    "    variance = self.running_variance / (self.num_steps + 1.0)\n",
    "    variance = torch.clip(variance, 1e-6, 1e6)\n",
    "    return ((observation - self.running_mean) / variance.sqrt()).clip(-5, 5)\n",
    "\n",
    "  @torch.jit.export\n",
    "  def get_logits_action(self, observation):\n",
    "    observation = self.normalize(observation)\n",
    "    logits = self.policy(observation)\n",
    "    loc, scale = self.dist_create(logits)\n",
    "    action = self.dist_sample_no_postprocess(loc, scale)\n",
    "    return logits, action\n",
    "\n",
    "  @torch.jit.export\n",
    "  def chamfer_loss(self, pred, target):\n",
    "        pred = pred.unsqueeze(2)  # (batch_size, seq_len, 1, feature_dim)\n",
    "        target = target.unsqueeze(1)  # (batch_size, 1, seq_len, feature_dim)\n",
    "        pred = pred.repeat(1, 1, target.shape[2], 1)\n",
    "        target = target.repeat(1, pred.shape[1], 1, 1)\n",
    "        distances = torch.sqrt(((pred - target) ** 2).mean(dim=-1))  # (batch_size, seq_len, seq_len)\n",
    "        min_distances, _ = distances.min(dim=-1)\n",
    "        return min_distances.mean()\n",
    "        \n",
    "  @torch.jit.export\n",
    "  def loss(self, td: Dict[str, torch.Tensor], demo_traj, demo_traj_action):\n",
    "      observation = self.normalize(td['observation'])\n",
    "      policy_logits = self.policy(observation)\n",
    "\n",
    "      # Compute rollout trajectory and demo trajectory normalization\n",
    "      rollout_traj = torch.cat([td['qp_list']['pos'].reshape((td['qp_list']['pos'].shape[0], td['qp_list']['pos'].shape[1], -1)),\n",
    "                                td['qp_list']['rot'].reshape((td['qp_list']['rot'].shape[0], td['qp_list']['rot'].shape[1], -1)),\n",
    "                                td['qp_list']['vel'].reshape((td['qp_list']['vel'].shape[0], td['qp_list']['vel'].shape[1], -1)),\n",
    "                                td['qp_list']['ang'].reshape((td['qp_list']['ang'].shape[0], td['qp_list']['ang'].shape[1], -1))], axis=-1)\n",
    "\n",
    "      self.update_normalization(observation)\n",
    "      demo_traj_ = self.normalize(demo_traj)\n",
    "\n",
    "      # Calculate state chamfer loss\n",
    "      cf_loss = self.chamfer_loss(rollout_traj, demo_traj_)\n",
    "\n",
    "      # Calculate action chamfer loss\n",
    "      pred_action = td['action_list']\n",
    "      pred_demo_action = demo_traj_action\n",
    "      cf_action_loss = self.chamfer_loss(pred_action, pred_demo_action)\n",
    "\n",
    "      # Calculate entropy loss\n",
    "      loc, scale = torch.chunk(policy_logits, 2, dim=-1)\n",
    "      sigma_list = F.softplus(scale) + 1e-6  # Small epsilon for numerical stability\n",
    "      entropy_loss = -0.5 * torch.log(2 * torch.pi * sigma_list ** 2).mean()\n",
    "\n",
    "      # Combine losses\n",
    "      final_loss = cf_loss + cf_action_loss + self.entropy_cost * entropy_loss\n",
    "      final_loss = torch.tanh(final_loss)\n",
    "\n",
    "      return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StepData = collections.namedtuple(\n",
    "    'StepData',\n",
    "    ('observation', 'logits', 'action', 'reward', 'done', 'truncation'))\n",
    "\n",
    "\n",
    "def sd_map(f: Callable[..., torch.Tensor], *sds) -> StepData:\n",
    "  \"\"\"Map a function over each field in StepData.\"\"\"\n",
    "  items = {}\n",
    "  keys = sds[0]._asdict().keys()\n",
    "  for k in keys:\n",
    "    items[k] = f(*[sd._asdict()[k] for sd in sds])\n",
    "  return StepData(**items)\n",
    "\n",
    "\n",
    "def eval_unroll(agent, env, length):\n",
    "  \"\"\"Return number of episodes and average reward for a single unroll.\"\"\"\n",
    "  observation = env.reset()\n",
    "  episodes = torch.zeros((), device=agent.device)\n",
    "  episode_reward = torch.zeros((), device=agent.device)\n",
    "  for _ in range(length):\n",
    "    _, action = agent.get_logits_action(observation)\n",
    "    observation, reward, done, _ = env.step(Agent.dist_postprocess(action))\n",
    "    episodes += torch.sum(done)\n",
    "    episode_reward += torch.sum(reward)\n",
    "  return episodes, episode_reward / episodes\n",
    "\n",
    "\n",
    "def train_unroll(agent, env, observation, num_unrolls, unroll_length):\n",
    "  \"\"\"Return step data over multple unrolls.\"\"\"\n",
    "  sd = StepData([], [], [], [], [], [])\n",
    "  for _ in range(num_unrolls):\n",
    "    one_unroll = StepData([observation], [], [], [], [], [])\n",
    "    for _ in range(unroll_length):\n",
    "      logits, action = agent.get_logits_action(observation)\n",
    "      observation, reward, done, info = env.step(Agent.dist_postprocess(action))\n",
    "      one_unroll.observation.append(observation)\n",
    "      one_unroll.logits.append(logits)\n",
    "      one_unroll.action.append(action)\n",
    "      one_unroll.reward.append(reward)\n",
    "      one_unroll.done.append(done)\n",
    "      one_unroll.truncation.append(info['truncation'])\n",
    "    one_unroll = sd_map(torch.stack, one_unroll)\n",
    "    sd = sd_map(lambda x, y: x + [y], sd, one_unroll)\n",
    "  td = sd_map(torch.stack, sd)\n",
    "  return observation, td\n",
    "\n",
    "\n",
    "def train(\n",
    "    env_name: str = 'ant',\n",
    "    num_envs: int = 2048,\n",
    "    episode_length: int = 1000,\n",
    "    device: str = 'cuda',\n",
    "    num_timesteps: int = 30_000_000,\n",
    "    eval_frequency: int = 10,\n",
    "    unroll_length: int = 5,\n",
    "    batch_size: int = 1024,\n",
    "    num_minibatches: int = 32,\n",
    "    num_update_epochs: int = 4,\n",
    "    reward_scaling: float = .1,\n",
    "    entropy_cost: float = 1e-2,\n",
    "    discounting: float = .97,\n",
    "    learning_rate: float = 3e-4,\n",
    "    progress_fn: Optional[Callable[[int, Dict[str, Any]], None]] = None,\n",
    "):\n",
    "\n",
    "  env = envs.create(env_name, batch_size=num_envs,\n",
    "                    episode_length=episode_length,\n",
    "                    backend='spring')\n",
    "  env = gym_wrapper.VectorGymWrapper(env)\n",
    "  # automatically convert between jax ndarrays and torch tensors:\n",
    "  env = torch_wrapper.TorchWrapper(env, device=device)\n",
    "\n",
    "  # env warmup\n",
    "  env.reset()\n",
    "  action = torch.zeros(env.action_space.shape).to(device)\n",
    "  env.step(action)\n",
    "\n",
    "  # create the agent\n",
    "  policy_layers = [\n",
    "      env.observation_space.shape[-1], 64, 64, env.action_space.shape[-1] * 2\n",
    "  ]\n",
    "\n",
    "  agent = Agent(policy_layers, entropy_cost, discounting,\n",
    "                reward_scaling, device)\n",
    "  agent = torch.jit.script(agent.to(device))\n",
    "  optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
    "\n",
    "  sps = 0\n",
    "  total_steps = 0\n",
    "  total_loss = 0\n",
    "\n",
    "  # Expert Demo\n",
    "  args.logdir = f\"logs/{args.env}/{args.env}_ep_len{args.ep_len}_num_envs{args.num_envs}_lr{args.lr}_trunc_len{args.trunc_len}\" \\\n",
    "                f\"_max_it{args.max_it}_max_grad_norm{args.max_grad_norm}_re_dis{args.reverse_discount}_ef_{args.entropy_factor}\" \\\n",
    "                f\"_df_{args.deviation_factor}_acf_{args.action_cf_factor}_l2loss_{args.l2}_il_{args.il}_ILD_{args.ILD}\" \\\n",
    "                f\"/seed{args.seed}\"\n",
    "  demo_traj = np.load(f\"expert/{args.env}_traj_state.npy\")\n",
    "  demo_traj = jnp.array(demo_traj)[:args.ep_len][:, None, ...].repeat(args.num_envs, 1)\n",
    "  demo_traj_action = np.load(f\"expert/{args.env}_traj_action.npy\")\n",
    "  demo_traj_action = jnp.array(demo_traj_action)[:args.ep_len][:, None, ...].repeat(args.num_envs, 1)\n",
    "  demo_traj_obs = np.load(f\"expert/{args.env}_traj_obs.npy\")\n",
    "  demo_traj_obs = jnp.array(demo_traj_obs)[:args.ep_len][:, None, ...].repeat(args.num_envs, 1)\n",
    "  reverse_discounts = jnp.array([args.reverse_discount ** i for i in range(args.ep_len, 0, -1)])[None, ...]\n",
    "  reverse_discounts = reverse_discounts.repeat(args.num_envs, 0)\n",
    "\n",
    "  for eval_i in range(eval_frequency + 1):\n",
    "    if progress_fn:\n",
    "      t = time.time()\n",
    "      with torch.no_grad():\n",
    "        episode_count, episode_reward = eval_unroll(agent, env, episode_length)\n",
    "      duration = time.time() - t\n",
    "      # TODO: only count stats from completed episodes\n",
    "      episode_avg_length = env.num_envs * episode_length / episode_count\n",
    "      eval_sps = env.num_envs * episode_length / duration\n",
    "      progress = {\n",
    "          'eval/episode_reward': episode_reward,\n",
    "          'eval/completed_episodes': episode_count,\n",
    "          'eval/avg_episode_length': episode_avg_length,\n",
    "          'speed/sps': sps,\n",
    "          'speed/eval_sps': eval_sps,\n",
    "          'losses/total_loss': total_loss,\n",
    "      }\n",
    "      progress_fn(total_steps, progress)\n",
    "\n",
    "    if eval_i == eval_frequency:\n",
    "      break\n",
    "\n",
    "    observation = env.reset()\n",
    "    num_steps = batch_size * num_minibatches * unroll_length\n",
    "    num_epochs = num_timesteps // (num_steps * eval_frequency)\n",
    "    num_unrolls = batch_size * num_minibatches // env.num_envs\n",
    "    total_loss = 0\n",
    "    t = time.time()\n",
    "    for _ in range(num_epochs):\n",
    "      observation, td = train_unroll(agent, env, observation, num_unrolls,\n",
    "                                     unroll_length)\n",
    "\n",
    "      # make unroll first\n",
    "      def unroll_first(data):\n",
    "        data = data.swapaxes(0, 1)\n",
    "        return data.reshape([data.shape[0], -1] + list(data.shape[3:]))\n",
    "      td = sd_map(unroll_first, td)\n",
    "\n",
    "      # update normalization statistics\n",
    "      agent.update_normalization(td.observation)\n",
    "\n",
    "      for _ in range(num_update_epochs):\n",
    "        # shuffle and batch the data\n",
    "        with torch.no_grad():\n",
    "          permutation = torch.randperm(td.observation.shape[1], device=device)\n",
    "          def shuffle_batch(data):\n",
    "            data = data[:, permutation]\n",
    "            data = data.reshape([data.shape[0], num_minibatches, -1] +\n",
    "                                list(data.shape[2:]))\n",
    "            return data.swapaxes(0, 1)\n",
    "          epoch_td = sd_map(shuffle_batch, td)\n",
    "\n",
    "        for minibatch_i in range(num_minibatches):\n",
    "          td_minibatch = sd_map(lambda d: d[minibatch_i], epoch_td)\n",
    "          loss = agent.loss(td_minibatch._asdict(), demo_traj, demo_traj_action)\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          total_loss += loss\n",
    "\n",
    "    duration = time.time() - t\n",
    "    total_steps += num_epochs * num_steps\n",
    "    total_loss = total_loss / (num_epochs * num_update_epochs * num_minibatches)\n",
    "    sps = num_epochs * num_steps / duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sps = []\n",
    "\n",
    "def progress(_, metrics):\n",
    "  if 'training/sps' in metrics:\n",
    "    train_sps.append(metrics['training/sps'])\n",
    "\n",
    "ppo.train(\n",
    "    environment=envs.create(env_name='ant', backend='spring'),\n",
    "    num_timesteps = 30_000_000, num_evals = 10, reward_scaling = .1,\n",
    "    episode_length = 1000, normalize_observations = True, action_repeat = 1,\n",
    "    unroll_length = 5, num_minibatches = 32, num_updates_per_batch = 4,\n",
    "    discounting = 0.97, learning_rate = 3e-4, entropy_cost = 1e-2,\n",
    "    num_envs = 2048, batch_size = 1024, progress_fn = progress)\n",
    "\n",
    "print(f'train steps/sec: {np.mean(train_sps[1:])}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
